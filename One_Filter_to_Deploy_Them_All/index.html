<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-weight:300;
    }

    h2 {
        font-weight: 300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<style>
  /* Style for the container */
  .button-container {
      display: flex;
      justify-content: center;
      gap: 20px;
      margin-top: 30px;
  }

  /* Remove default link styles within buttons */
  .button-container a {
            text-decoration: none; /* Remove underline */
            color: #FFFFFF; /* White text */
        }

  /* Style for each button */
  .button {
      background-color: #4b4b4b; /* dark grey */
      color: #ffffff;
      border: none;
      padding: 12px 24px;
      font-size: 16px;
      text-align: center;
      text-decoration: none;
      border-radius: 25px; /* Rounded corners */
      transition: background-color 0.3s;
  }

  /* Hover effect for buttons */
  .button:hover {
      background-color: #818181; /* lighter grey */
  }
</style>

<style>
  /* Align the SVG with the text and resize it */
  .svg-icon {
      vertical-align: text-bottom; /* Align with the middle of the text */
      height: 1.5em; /* Make the height equal to the height of the text */
      width: auto; /* Maintain aspect ratio */
  }
  .icon-larger {
    font-size: 22px;
  }
</style>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

<html>
  <head>
        <title>One Filter to Deploy Them All</title>
        <meta property="og:title" content="Factored 3D" />
        <link rel="icon" href="./resources/images/favicon.png" type="image/png">
  </head>

  <body>
    <br><br>
    <center>
    <span style="font-size:42px">One Filter to Deploy Them All: Robust Safety for Quadrupedal Navigation in Unknown Environemnts</span>
    </center>

    <br>
      <table align=center width=900px>
       <tr>
        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://www.linkedin.com/in/albertkuilin/">Albert Lin<sup>1,2</sup></a></span>
        </center>
        </td>

        <td align=center width=100px>
          <center>
          <span style="font-size:20px"><a href="https://www.psrobotics.tech/">Shuang Peng<sup>1</sup></a></span>
          </center>
          </td>

        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://smlbansal.github.io/">Somil Bansal<sup>2</sup></a></span>
        </center>
        </td>
     </tr>
    </table>

    <br>
    <table align=center width=700px>
       <tr>
        <td align=center width=100px>
        <center>
        <span style="font-size:15px"><sup>1</sup> University of Southern California </span>
        </center>
        </td>
        <td align=center width=100px>
          <center>
          <span style="font-size:15px"><sup>2</sup> Stanford University </span>
          </center>
          </td>
     </tr>
    </table>

    <div class="button-container">
      <a href="./resources/One_Filter_to_Deploy_Them_All.pdf" class="button" target="_blank"><svg class="svg-icon" fill="white" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--!Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d="M64 464l48 0 0 48-48 0c-35.3 0-64-28.7-64-64L0 64C0 28.7 28.7 0 64 0L229.5 0c17 0 33.3 6.7 45.3 18.7l90.5 90.5c12 12 18.7 28.3 18.7 45.3L384 304l-48 0 0-144-80 0c-17.7 0-32-14.3-32-32l0-80L64 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16zM176 352l32 0c30.9 0 56 25.1 56 56s-25.1 56-56 56l-16 0 0 32c0 8.8-7.2 16-16 16s-16-7.2-16-16l0-48 0-80c0-8.8 7.2-16 16-16zm32 80c13.3 0 24-10.7 24-24s-10.7-24-24-24l-16 0 0 48 16 0zm96-80l32 0c26.5 0 48 21.5 48 48l0 64c0 26.5-21.5 48-48 48l-32 0c-8.8 0-16-7.2-16-16l0-128c0-8.8 7.2-16 16-16zm32 128c8.8 0 16-7.2 16-16l0-64c0-8.8-7.2-16-16-16l-16 0 0 96 16 0zm80-112c0-8.8 7.2-16 16-16l48 0c8.8 0 16 7.2 16 16s-7.2 16-16 16l-32 0 0 32 32 0c8.8 0 16 7.2 16 16s-7.2 16-16 16l-32 0 0 48c0 8.8-7.2 16-16 16s-16-7.2-16-16l0-64 0-64z"/></svg>  Paper</a>
      <a href="#" class="button" target="_blank"><span class="icon"><i class="icon-larger ai ai-arxiv"></i></span>  arXiv</a>
      <a href="#" class="button" target="_blank"><span class="icon"><svg class="svg-icon svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com --></span>  Code</a>
    </div>

    <br>

    <img src="./resources/images/overview.png" width="100%" height="auto">

    <br>
    <center><h1>Abstract</h1></center>
    As learning-based methods for legged robots rapidly grow in popularity, it is important that we can provide safety assurances efficiently across different controllers and environments.
Existing works either rely on <i>a priori</i> knowledge of the environment and safety constraints to ensure system safety or provide assurances for a specific locomotion policy.
To address these limitations, we propose an observation-conditioned reachability-based (OCR) safety-filter framework.
Our key idea is to use an OCR value network (OCR-VN) that predicts the optimal control-theoretic safety value function for new failure regions and dynamic uncertainty during deployment time.
Specifically, the OCR-VN facilitates rapid safety adaptation through two key components: a LiDAR-based input that allows the dynamic construction of safe regions in light of new obstacles and a disturbance estimation module that accounts for dynamics uncertainty in the wild.
The predicted safety value function is used to construct an adaptive safety filter that overrides the nominal quadruped controller when necessary to maintain safety.
Through simulation studies and hardware experiments on a Unitree Go1 quadruped, we demonstrate that the proposed framework can automatically safeguard a wide range of hierarchical quadruped controllers, adapts to novel environments, and is robust to unmodeled dynamics <i>without a priori access to the controllers or environments</i> - hence, ``One Filter to Deploy Them All''.
    <br>

    <center><h1>Approach</h1></center>

    <center><h1>Adapting to New Obstacles</h1></center>

    <center><h1>Adapting to Dynamics Uncertainty</h1></center>

    <center><h1>Safeguarding Different Controllers</h1></center>
   
    <!--
    <table align=center>
            <tr>
                <td width=1000>
                  <center>
                    <!- - METHOD VIDEO HERE - ->
                      <iframe width="840" height="473" src="#" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </center>
                </td>
            </tr>
    </table>
    -->
        

            <br>

            <br>
            <!-- <img src="./resources/images/Graphical_Abstract.jpg"> -->
            <br>
          <hr>

            <br>
            Learning-based approaches for controlling safety-critical autonomous systems are rapidly growing
            in popularity; thus, it is important to provide rigorous and robust assurances on their performance
            and safety. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for
            providing such guarantees, since it can handle general nonlinear system dynamics, bounded adversarial
            system disturbances, and state and input constraints. However, it involves solving a Partial
            Differential Equation (PDE), whose computational and memory complexity scales exponentially
            with respect to the state dimension, making its direct use on large-scale systems intractable. To
            overcome this challenge, neural approaches, such as DeepReach, have been used to synthesize
            reachable tubes and safety controllers for high-dimensional systems. However, verifying these
            neural reachable tubes remains challenging. In this work, we propose two different verification
            methods, based on robust scenario optimization and conformal prediction, to provide probabilistic
            safety guarantees for neural reachable tubes. Our methods allow a direct trade-off between resilience
            to outlier errors in the neural tube, which are inevitable in a learning-based approach, and
            the strength of the probabilistic safety guarantee. Furthermore, we show that split conformal prediction,
            a widely used method in the machine learning community for uncertainty quantification,
            reduces to a scenario-based approach, making the two methods equivalent not only for verification
            of neural reachable tubes but also more generally. To our knowledge, our proof is the first in the
            literature to show a strong relationship between the highly related but disparate fields of conformal
            prediction and scenario optimization. Finally, we propose an outlier-adjusted verification approach
            that harnesses information about the error distribution in neural reachable tubes to recover greater
            safe volumes. We demonstrate the efficacy of the proposed approaches for the high-dimensional
            problems of multi-vehicle collision avoidance and rocket landing with no-go zones.
            <br><br>
          <hr>
          <!-- <center><h1>Video</h1></center>
          <iframe width="960" height="540" src="https://youtube.com/embed/L7HuwDb2MLQ"></iframe> -->
          <hr>
          <!-- <center><h1>Poster</h1></center>
          <a href="./resources/Poster.pdf"><img style="height:280px" src="./resources/images/Poster_Image.jpg"/></a> -->
          <hr>
         <!-- <table align=center width=550px> -->
            <table align=center width=650>
             <center><h1>Paper</h1></center>
                <tr>
                    <!--<td width=300px align=left>-->
                    <!-- <a href="http://arxiv.org/pdf/1603.08511.pdf"> -->
                  <!-- <td><a href="#"><img class="layered-paper-big" style="height:175px" src="./resources/images/paper.png"/></a></td> -->
                  <td><a href="./resources/Verification_of_Neural_Reachable_Tubes.pdf"><img style="height:280px" src="./resources/images/paper_preview.jpg"/></a></td>
                  <td><span style="font-size:14pt">Lin, Bansal<br><br>
                          Verification of Neural Reachable Tubes <br> via Scenario Optimization and Conformal Prediction
                  <!-- [hosted on <a href="#">arXiv</a>]</a> -->
                    </td>
              </tr>
            </table>
          <br>

          <table align=center width=180px>
              <tr>
                  <td><span style="font-size:14pt"><center>
                      <a href="./resources/Verification_of_Neural_Reachable_Tubes.pdf">[pdf]</a>
                    </center></td>

                  <!-- <td><span style="font-size:14pt"><center>
                      <a href="./resources/bibtex.bib">[Bibtex]</a>
                    </center></td> -->
              </tr>
            </table>
              <br>

                <hr>

         <!-- <center><h1>Code</h1></center>
            <table align=center width=1000px>
                <tr>
                        <center>
                          <a href=''><img class="round" style="height:400" src="./resources/images/framework.png"/></a>
                        </center>
              </tr>
          </table>

            <table align=center width=800px>
              <tr><center> <br>
                <span style="font-size:28px">&nbsp;<a href='https://github.com/sia-lab-git/deepreach-verification'>[GitHub]</a>

                <span style="font-size:28px"></a></span>
              <br>
              </center></tr>
          </table>
            <br>
          <hr> -->

          <!-- 
          <center><h1>Results</h1></center>
          <table align=center width=900px>
              <tr>
                  <td width=600px>
                    <center>
                        <a href="./resources/images/results_simulation.png"><img src = "./resources/images/results_simulation.png" width="800px"></img></href></a><br>
                  </center>
                  </td>
              </tr>
                  <td width=600px>
                    <center>
                        <span style="font-size:14px"><i> <span style="font-weight:bold">Metrics</span> We evaluate LB-WayPtNav against a comparable end-to-end method and a purely geometric method which uses depth images to estimate an occupancy map of the environment for use in planning via Model Predictive Control (MPC). For the geometric method we compare against both an agent with memory which estimates an occupancy grid based on all the depth images thus far and one that is reactive and estimates an occupancy grid based only on its current view of the environment. All methods are tested on a set of 185 navigational goals in a previously unseen test environment. Our method is approximately 20-25% more successful on these new goals than the end-to-end method. The success rate of LB-WayPtNav, which itself is reactive, is comparable to that of the Mapping (memoryless) baseline. The remaining three metrics are computed on the subset of test goals on which all methods are successful. We find that the model based method navigates to the goal region 1.5 to 2 times as quickly as the end-to-end method with average acceleration approximately half that of the end-to-end method and average jerk approximately 1/20th that of the end-to-end method. LB-WayPtNav is comparable to the mapping based methods in terms of time to reach goal, acceleration, and jerk.</i>
                  </center>
                  </td>
              </tr>
          </table>
          <br>
          <table align=center width=900px>
              <tr>
                  <td width=600px>
                    <center>
                        <a href="./resources/images/velocity_profile.png"><img src = "./resources/images/velocity_profile.png" height="220px"></img></href></a><br>
                  </center>
                  </td>
              </tr>
                  <td width=600px>
                    <center>
                        <span style="font-size:14px"><i> <span style="font-weight:bold">Control Profiles</span> The proposed method produces significantly smoother control profiles than the end-to-end method which are much easier to track on a real physical system. The jerky profiles learned by the end-to-end method could lead to increased probability of hardware failure on a real system as well. Additionally, these jerky control profiles would lead to dramatically increased power usage and thus decreased battery life.</i>
                  </center>
                  </td>
              </tr>
          </table>
          <br>
          <table align=center width=900px>
              <tr>
                  <td width=600px>
                    <center>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/Eny_WAynRms" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>
                  </center>
                  </td>
              </tr>
                  <td width=600px>
                    <center>
                        <span style="font-size:14px"><i> <span style="font-weight:bold">Learned Visual Semantics</span> We find that, through the training process, the agent learns generalizable visual semantics of indoor<br> office environments (i.e. exiting a room by first locating the door and then moving through it, going around a chair that is only partially<br> visible, locating and moving into a hallway to then move into a room connected to this hallway). The agent exhibits this behavior <br>in visually diverse scenarios where providing such supervision explicitly can be very difficult.</i>
                  </center>
                  </td>
              </tr>
          </table>
          <hr>
         
          <center><h1>Hardware Experiments</h1></center>
          <!- - <center> - ->
            
            <br>
            <table align=center width=900px>
              <tr>
                  <td align=center>
                    <a href="./resources/images/front_fig_comp.png"><img src = "./resources/images/front_fig_comp.png" height=200px></img></href></a><br>
                  </td>
                  <td>
                    <p>We deploy our simulation trained algorithm on a Turtlebot 2 to test on real-world navigational scenarios. Each experiment is visualized from three different viewpoints, however the robot only sees the "First Person View" (also labeled Robot View). The other two viewpoints are provided for context only. We do not train or finetune our algorithm in any way on real data. All experiments are shown in realtime.</p>
                  </td>
              </tr>
            </table>
            <br>
            <br>
              <table align=center width=900px>
              <tr>
                  <td width=600px>
                    <center>
                        <a href="./resources/images/results_experiments.png"><img src = "./resources/images/results_experiments.png" width="800px"></img></href></a><br>
                  </center>
                  </td>
              </tr>
                  <td width=600px>
                    <center>
                        <span style="font-size:14px"><i> <span style="font-weight:bold">Metrics</span> In real-world experiments LB-WayPtNav continues to perform well, however performance of the mapping based methods degrades considerably due to errors in depth estimation on shiny/matte objects (bike frames, tires, computer monitors, etc.), on thin/intricate objects (power cables, chair legs), and in the presence of strong infared light (sunlight).</i>
                  </center>
                  </td>
              </tr>
          </table>
          <!- - </center> - ->
          <br> 

          <center><h2>Videos</h2></center>
          <table align=center>
            <tr>
              <td>
                <center>
                      <iframe width="373" height="210" src="#" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>
                </center>
              </td>

              <td>
                <center>
                      <iframe width="373" height="210" src="#" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>
                </center>
              </td>
            </tr>

            <tr>
              <td>
                <center>
                      <iframe width="373" height="210" src="#" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>
                </center>
              </td>

              <td>
                <center>
                      <iframe width="373" height="210" src="#" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>
                </center>
              </td>

            </tr>
          </table>

          <table align=center>
            <tr>
              <td>
                <center>
                      <iframe width="373" height="210" src="#" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>
                </center>
              </td>
            </tr>
            <td width=800px>
                <center>
                    <span style="font-size:14px"><i> <span style="font-weight:bold">---</span> ---</i>
                    <br><br><br>
                </center>
              </td>
          </table>

              <hr>
              <table align=center width=1000px>
              <tr>
                <td width=800px>
                  <center>
                    <h1>---</h1>
                  </center>
                </td>
              </tr>
              <tr>

                  <td width=800px>
                    <center>
                        <iframe width="560" height="315" src="https://youtu.be/L7HuwDb2MLQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>
                  </center>
                  </td>
              </tr>
              <td width=600px>
                    <center>
                        <span style="font-size:14px"><i>---</i>
                        <br><br><br>
                  </center>
                  </td>
          </table>


           <hr>
           -->

            <table align=center width=1100px>
                <tr>
                    <td>
                      <left>
                <center><h1>Acknowledgements</h1></center>
                This work is supported in part by a NASA Space Technology Graduate Research Opportunity, the NSF CAREER Program under award 2240163, and the DARPA ANSR program.
                <br>
                ---
                <br>
                This webpage template was borrowed from <a href="https://agile-but-safe.github.io/">Agile But Safe</a>.
            </left>
        </td>
        </tr>
        </table>

        <br><br>
</body>
</html>
